---
title: "Final Project Document"
author: "Duyen Ho, Michael Garbus, Christina Hu"
date: "12/6/2021"
output:
  pdf_document:
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
urlcolor: blue
---


\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Description and Summary (NEED GOAL, APPROACH, and CONCLUSION AND THIS NEEDS TO BE ONE PAGE?!!?)

The goal of this project is to find the most accurate model using a variety of statistical model building techniques in order to predict medical outcomes relating to breast cancer. We will use the BRCA Multi-Omics (TCGA) data from Kaggle, and predict outcomes for the `PR.Status`, `ER.Status`, `HER2.Final.Status`, and `histological.type`. For our approach, we will first perform the necessary data cleaning operations, then [method 1] and [method2] to build classification models for `PR.Status` and [method1] and [method2] for `histological.type`, using classification error and AUC respectively as the evalution criteria. Next, we will use [method] to build a model with the goal of accurately predicting **all four** outcomes. Finally, [INSERT CONCLUSION HERE].

- can include any difficulties or describe outcomes for models

# Literature Review

We consulted some research involving the subject to guide us in our modelling. The Wisconsin Breast Cancer dataset was used, which involves a digitized image of a fine needle aspirate (FNA) of a breast mass, which describe the characteristics of the cell nuclei present in the image. Unfortunately, this is rather different from our current data, which contains a sample of genetic data instead of a picture of cell data. However, we still feel that their approaches can be useful for our data, as they are predicting binary outcomes on a similar subject matter.

The first study considered was "Breast Cancer Prediction: A Comparative Study Using Machine
Learning Techniques", published in 2020 by Islam, M.M. et al. in SN Computer Science.  The paper compares 5 different supervised machine learning techniques (support vector machine, K-nearest neighbors, random forests, artificial neural networks, and finally, logistic regression.)  They performed little preprocessing on the data, as they only removed 16 NA variables. They divided the data into test and train sets, and used a 10-fold cross validation, with nine-fold used for training and the remaining fold used for testing. The results revealed that ANNs return the highest accuracy ($98.57\%$), KNN and SVM came second ($97.1\%$), and random forests and logistic regression came third, clocking in with an accuracy of $95.71\%$ for the two models. ANNs also achieved the highest specificity ($96\%$), whereas SVM achieved the lowest, at around $92.3\%$. KNN and random forest had a specificity of $98.53\%$, whereas logistic regression had a specificity of $95.65\%$. ANN and SVM achieved the highest sensitivity, clocking in at exactly $100\%$. This made SVM and ANN very attractive to us when considering our model selection. KNN had a sensitivity of $97.82\%$, Random Forest had a sensitivity of $95.65\%$, and logistic regression had a sensitivity of $95.74\%$.

However, the highest values for AUC were KNN and a random forest model, both achieving an AUC of $99\%$. 

In the discussion of related works, the authors mentioned another study which received a $98.83%$ accuracy for a boosted trees random forest model, which inspired us to test gradient boosted trees for the final part of the project, as they will be able to determine which features are important, and hopefully cut down on features as well.

A similar study, "Using Machine Learning Algorithms for Breast Cancer Risk
Prediction and Diagnosis", published in 2016 by Asri,H. et al in Procedia Computer Science 83 tested SVM, decision tree (C 4.5, a technique available in the open source WEKA data analysis tool), Naive Bayes, as well as KNN on the same UCI Breast Cancer dataset, with a focus on accuracy. Asri et al. discovered that although SVM took the longest time to create a model, it resulted in the highest accuracy of $97.54\%$,  which is a different result to the previous model we studied. 

While I do not believe that time constraints will be important in regards to our model as we are trying to achieve a high level of classification, it is interesting that these values are different. I imagine that this may be due to a difference in tools used, or perhaps, in the kernel used for the SVM classification. Additionally, the study also found that SVM had the highest AUC value on the ROC curve ($99\%$). For these reason, we decided to use SVM for our model.

For the creation of our final model, we consulted some literature on predicting in medical data. We found "Predicting Missing Values in Medical Data via XGBoost Regression", from Zhang et al. Because of this model, as well as discussion in Islam et al., we decided to consider XGBoost. Fortunately, XGBoost is able to use AUC values as its objective function. We decided to use a multi-label machine learning tool, `utiml`, so we would be able to predict all four labels from our 50 predictors. Serendipitously, this library is very similar to the WEKA tool used in Asri et al. which led us to consider using SVM for this application as well. Fortunately, the `utiml` package uses the package we used for SVM in this class, `e1071`. After further analysis, SVM was selected.

# Summary Statistics and Data Processing 

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
# head(brca_data, 10)
```

•	705 observations (breast cancer samples)

•	1936 variables/predictors (4 different omics data types)

#[1:604] are rn - gene expressions 604 <- categorical data

#[605:1464] are cn - copy number variations 860 

#[1465:1713] are mu - somatic mutations 249 <- categorical data

#[1714:1936] are pp - protein expression 223

•	4 outcomes [1937:1940] 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
brca_org = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/mgarbus2/brca_data_w_subtypes.csv")
```

## Data Cleaning

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#remove the 'vital.status'
brca_org <- brca_org[,-1937]
brca_org[(brca_org$histological.type == "infiltrating lobular carcinoma") | (brca_org$histological.type == "infiltrating ductal carcinoma") | (brca_org$HER2.Final.Status == "Positive") | (brca_org$HER2.Final.Status == "Negative") | (brca_org$ER.Status == "Positive") | (brca_org$ER.Status == "Negative") | (brca_org$PR.Status == "Positive") | (brca_org$PR.Status == "Negative")]
brca_org[1:5,1935:1940]
#cleaned data
brca_data  = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
```

•	There was no missing value in the entire dataset, so there’s no missing values in the continuous predictors

```{r, echo=FALSE, message=FALSE, warning=FALSE}
sum(colSums(is.na(brca_org)))
```

•	There were some outliers in the continuous predictors, I just did a quick test on the cn variables. But we decided to keep them as they are and filter out the dataset later on with the correlation matrix

```{r, echo=FALSE, message=FALSE, warning=FALSE}
outfun <- function(x) {
  abs(x-mean(x,na.rm=TRUE)) > 3*sd(x,na.rm=TRUE)
}
sum(outfun(brca_org[,605]))
sum(outfun(brca_org[,1000]))
```

•	My group wants to focus on filtering out the unsignificant continuous variables first, so we used correlation matrix method. Then we drop the columns that are highly correlated.

#Use correlation matrix

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(caret))
suppressMessages(library(dplyr))
# store the predictors
predictors = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# only use variances using the corr mat
non_categorical_brca = cbind(brca_data[1:604], brca_data[1714:1936])
# use correlation matrix
cor_mat = cor(non_categorical_brca)
# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
non_categorical_brca =  non_categorical_brca[, -c(cor_list)]
```


# Modeling PR.Status

## Modeling with KNN (predict PR.Status)

First, we will use KNN model to predict `PR.Status`. We will use a test train split of 25/75. We will only be using the non-categorical variables to build the KNN model since we ran into a lot of errors trying to incorporate the categorical variables

We will find the optimal k-value to use, ranging from k=1-10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(caret)
library(class)
# set the seed to make partition reproducible
set.seed(432)
k_vals = c(1:10)
err_vals = rep(NA, length(k_vals))
## 75% of the sample size
smp_size <- floor(0.75 * nrow(non_categorical_brca))
for(i in 1:length(k_vals)) {
  
  train_ind <- sample(seq_len(nrow(non_categorical_brca)), size = smp_size)
  brca_train <- as.matrix(non_categorical_brca[train_ind, ])
  y = as.matrix(predictors$PR.Status[train_ind])
  
  brca_test <- as.matrix(non_categorical_brca[-train_ind, ])
  pred = knn(train = brca_train, test = brca_test, cl = y, k = i)
  cmat = table(pred, as.factor(predictors$PR.Status[-train_ind]))
  # calculate misclassification rate
  err_vals[i] = (cmat[1,2]+cmat[2,1])/sum(cmat)
}
```

Next, we will plot the graph of the classification errors for each k-value and see if we can use the result to determine the best k:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(k_vals, err_vals, type = "l", xlab = "k-value", ylab = "Classification Error")
title("Classification Error per K-value")
```

As shown from the plot, the k-value with the lowest classification error is k=5, so we will use that for our model.

Next, use K=5 to create our KNN model and display the confusion matrix:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(432)
train_y = as.matrix(predictors$PR.Status[train_ind])
y = as.matrix(predictors$PR.Status[train_ind])
pred = knn(train = brca_train, test = brca_test, cl = y, k = 5)
cmat = table(pred, as.factor(predictors$PR.Status[-train_ind]))
cmat
```

Displaying the accuracy:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
(cmat[1,1]+cmat[2,2])/sum(cmat)
```
The classificaton error for a model with k=5 is 1-`r (cmat[1,1]+cmat[2,2])/sum(cmat)` = `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)`.


## Modeling with Lasso (predict PR.Status)

Next, we will create a Lasso model with 10-fold cross-validation to help create `PR.Status`:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(glmnet))
set.seed(432)
lasso.fit = cv.glmnet(brca_train, train_y, nfolds = 10, alpha = 1, type.measure = "class", family = "binomial")
```

Plotting the fit of our Lasso model:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow = c(1, 2))
    plot(lasso.fit)
    plot(lasso.fit$glmnet.fit, "lambda")
```

We will use the `best_lambda` to use for the prediction on the test data:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
best_lambda = lasso.fit$lambda.min
test_predict = predict(lasso.fit, brca_test, s = best_lambda, type = "class")
```

Displaying the confusion matrix of the results:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
cmat_1 = table(predictors$PR.Status[-train_ind], test_predict)
cmat_1
```

Displaying the accuracy:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
(cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)
```
The classification error is 1-`r (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)` = `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`.


### `PR.Status` Summary:

To summarize the model predictions for `PR.Status`, we used KNN and Lasso models to predict the outcome. We only used non-categorical data for both instances since the categorical variables gave many errors while we were in the process of fitting the models. However, even without the categorical predictors, both models yielded relatively low classification errors, with the KNN model (using k=8) having a classification error of `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)` and the Lasso model having a classification error of `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`. Looking at these results, the Lasso model would be better suited to predict `PR.Status`.


# Modelling `histological.type`

Next, let us model `histological.type`, using AUC as the evaluation criterion. For both models that we chose to model `histological.type` with, we decided to use ALL of the categorical and non-categorical predictors to see if we could get a more accurate result. 

## Modelling `histological.type` with SVM

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# SVM, histological type
library(ROCR)
library(e1071)
library(caret)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
non_response_brca <- brca_data[,-c(which(colnames(brca_data) %in% c('PR.Status','ER.Status','HER2.Final.Status','histological.type')))]
set.seed(432)
#SVM WITH MATCORR DATA
# use correlation matrix
cor_mat = cor(non_response_brca)
# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
non_response_brca <- non_response_brca[,cor_list]
#New BRCA data
train = brca_data$histological.type
train_data_unsplit = cbind(train,non_response_brca)
#continue to use the combined 10 data? or use the full data
#train_data_unsplit[,'train']  1 and 2
#80 20 split
train_rows <- sample(nrow(train_data_unsplit), size = floor(0.8 *nrow(train_data_unsplit)))
train_data <- train_data_unsplit[train_rows,]
test_data = train_data_unsplit[-train_rows,]
#Let's consider a few different kernels
#Sigma is the distance from the boundary
#Do we want a high cost? Or a low one?
svm.radial <- train(train ~ ., data = train_data, method = "svmRadial",
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1), sigma = c(1, 2, 3)),
                trControl = trainControl(method = "cv", number = 5))
svm.linear <- train(train ~ ., data = train_data, method = 'svmLinear',
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1)),
                trControl = trainControl(method = "cv", number = 5))
```

Let us display the SVM results for radial and linear models respectively:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
svm.radial
svm.linear
```

Next, we will use each model to predict on the test data:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(kableExtra))
linear_svm_prediction <- predict(svm.linear, test_data[,-1])
radial_svm_prediction <- predict(svm.radial, test_data[,-1])
#Radial --> unable to classify any infiltrating lobular carcinoma. Linear is the better model
suppressMessages(library(ROCR))
lin.svm <- prediction(as.numeric(as.factor(linear_svm_prediction)), as.factor(test_data[,1]))
rad.svm <- prediction(as.numeric(as.factor(radial_svm_prediction)), as.factor(test_data[,1]))
```

Finally, let us display the results of the Linear and Radial SVM AUC:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
# Linear has a performance of about 65.98%
performance(lin.svm,measure = "auc")@y.values[[1]]
# Radial has performance of about 50%
performance(rad.svm,measure = "auc")@y.values[[1]]
```

Looking at the results above, Linear SVM has AUC = `r performance(lin.svm,measure = "auc")@y.values[[1]]` and Radial SVM has AUC = `r performance(rad.svm,measure = "auc")@y.values[[1]]`. We can conclude that Linear SVM is probably the better model due to this significant difference.

## Modelling `histological.type` with Random Forests

Next, let us use a Random Forest model:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
train_y_hist= as.matrix(predictors$histological.type[train_ind])
brca_train_2 = as.matrix(cbind(brca_train, train_y_hist))
```

Let us fit the model with `nodesize=8` and `sampsize=50`. Through multiple rounds of experimentation with values, we found those parameters to be most optimal:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
suppressMessages(library(randomForest))
set.seed(432)
rf.fit = randomForest(data.matrix(brca_train_2[, -c(640)]), y = as.factor(brca_train_2[, 640]), ntree = 4, mtry = 4, nodesize = 8, sampsize = 50)
```

Next, use this model to predict on the test data:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
test.predictions <- predict(rf.fit, type="prob", newdata=brca_test)[,2]
correct_hist = as.factor(predictors$histological.type[-train_ind])
rf_pr_test <- prediction(test.predictions, correct_hist)
r_auc_test <- performance(rf_pr_test, "tpr","fpr")
plot(r_auc_test, colorize=TRUE)
```

The AUC:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
performance(rf_pr_test, measure = "auc")@y.values[[1]] 
```
The `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. 

### `histological.type` Summary:

To summarize our models, we used Linear and Radial SVM, with had AUC values of `r performance(lin.svm,measure = "auc")@y.values[[1]]` and `r performance(rad.svm,measure = "auc")@y.values[[1]]` respectively. For random forest, the AUC value resulted in `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. Therefore, the random forest model can be determined as the better model, although its AUC score is still a bit low.


# 50-Variable Selection


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(utiml)
library(mldr)
library(kableExtra)
# Create two partitions (train and test) of toyml multi-label dataset
#with_factors 
factor_data <- cbind(non_response_brca[1:50],response_data_n)
my_mldr <- mldr_from_dataframe(factor_data, labelIndices = c(51:54))
#Keep as BR to ignore between labels!!!
svm_results <- cv(my_mldr, br, base.algorith="SVM", seed=432, kernel = "linear", cost = 0.6, cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)
xgb_results <- cv(my_mldr, br, base.algorith="XGB", seed=432, cores = 1, eta = 0.3, nrounds = 50, eval_metric = "auc", cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)
svm_final_result <-rbind(svm_results$multilabel[,c(1,5)],colMeans(svm_results$multilabel[,c(1,5)]))
svfr <- as.table(svm_final_result)
kable(svfr)
```

