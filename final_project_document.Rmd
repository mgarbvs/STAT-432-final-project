---
title: "Classification of cancer status based on genetic markers using various unsupervised learning, classification and regression methods"
author: "Duyen Ho, Michael Garbus, Christina Hu"
date: "12/6/2021"
output:
  pdf_document:
    includes:
      in_header: "header.tex"
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
urlcolor: blue
---

```{r, echo=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


```{css, echo=FALSE}
h1, h4 {
  text-align: center;
}
```

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Description and Summary (NEED GOAL, APPROACH, and CONCLUSION AND THIS NEEDS TO BE ONE PAGE?!!?)

The goal of this project is to find the most accurate model using a variety of statistical model building techniques in order to predict medical outcomes relating to breast cancer. We will use the BRCA Multi-Omics (TCGA) data from Kaggle, and predict outcomes for the `PR.Status`, `ER.Status`, `HER2.Final.Status`, and `histological.type`. 

For our approach, we will first perform the necessary data cleaning operations, then we will use **KNN** and **Lasso** methods to build classification models for `PR.Status` and **SVM** and **Random Forests** for `histological.type`, using classification error and AUC respectively as the evalution criteria. Next, we will have **Linear** and **Radial SVM** along with **XGBoost** to build a model with the goal of accurately predicting **all four** outcomes narrowing the predictors down to 50 variables. 

Finally, we have concluded that Lasso was a better method to fit the model when predicting `PR.Status`. On the other hand, Random Forest gives us a higher AUC value, so this would best predict our `histological.type` variable.
And for our last part of choosing 50 variables to predict all 4 outcomes, XGBoost helps us estimate the importance of features for a predictive modeling problem using the gradient boosting algorithm. With this method, we are confident that our 50 predictors influence our outcomes the most. This way, our model doesn't have noise from unimportant/irrelevant variables that we have filtered out.

Some challenges we faced were that the categorical variables gave us many errors when we were trying to fit some of the models, so we excluded them while modeling those potential fits first.

# Literature Review

We consulted some research involving the subject to guide us in our modelling. The Wisconsin Breast Cancer dataset was used, which involves a digitized image of a fine needle aspirate (FNA) of a breast mass, which describe the characteristics of the cell nuclei present in the image. Unfortunately, this is rather different from our current data, which contains a sample of genetic data instead of a picture of cell data. However, we still feel that their approaches can be useful for our data, as they are predicting binary outcomes on a similar subject matter.

The first study considered was "Breast Cancer Prediction: A Comparative Study Using Machine
Learning Techniques", published in 2020 by Islam, M.M. et al. in SN Computer Science.  The paper compares 5 different supervised machine learning techniques (support vector machine, K-nearest neighbors, random forests, artificial neural networks, and finally, logistic regression).  They performed little preprocessing on the data, as they only removed 16 NA variables. They divided the data into test and train sets, and used a 10-fold cross validation, with nine-fold used for training and the remaining fold used for testing. The results revealed that ANNs return the highest accuracy ($98.57\%$), KNN and SVM came second ($97.1\%$), and random forests and logistic regression came third, clocking in with an accuracy of $95.71\%$ for the two models. ANNs also achieved the highest specificity ($96\%$), whereas SVM achieved the lowest, at around $92.3\%$. KNN and random forest had a specificity of $98.53\%$, whereas logistic regression had a specificity of $95.65\%$. ANN and SVM achieved the highest sensitivity, clocking in at exactly $100\%$. This made SVM and ANN very attractive to us when considering our model selection. KNN had a sensitivity of $97.82\%$, Random Forest had a sensitivity of $95.65\%$, and logistic regression had a sensitivity of $95.74\%$.

However, the highest values for AUC were KNN and a random forest model, both achieving an AUC of $99\%$. 

In the discussion of related works, the authors mentioned another study which received a $98.83\%$ accuracy for a boosted trees random forest model, which inspired us to test gradient boosted trees for the final part of the project, as they will be able to determine which features are important, and hopefully cut down on features as well.

A similar study, "Using Machine Learning Algorithms for Breast Cancer Risk
Prediction and Diagnosis", published in 2016 by Asri,H. et al in Procedia Computer Science 83 tested SVM, decision tree (C 4.5, a technique available in the open source WEKA data analysis tool), Naive Bayes, as well as KNN on the same UCI Breast Cancer dataset, with a focus on accuracy. Asri et al. discovered that although SVM took the longest time to create a model, it resulted in the highest accuracy of $97.54\%$,  which is a different result to the previous model we studied. 

While we do not believe that time constraints will be important in regards to our model as we are trying to achieve a high level of classification, it is interesting that these values are different. We imagine that this may be due to a difference in tools used, or perhaps, in the kernel used for the SVM classification. Additionally, the study also found that SVM had the highest AUC value on the ROC curve ($99\%$). For these reason, we decided to use SVM for our model.

For the creation of our final model, we consulted some literature on predicting in medical data. We found "Predicting Missing Values in Medical Data via XGBoost Regression", from Zhang et al. Because of this model, as well as discussion in Islam et al., we decided to consider XGBoost. Fortunately, XGBoost is able to use AUC values as its objective function. We decided to use a multi-label machine learning tool, `utiml`, so we would be able to predict all four labels from our 50 predictors. Serendipitously, this library is very similar to the WEKA tool used in Asri et al. which led us to consider using SVM for this application as well. Fortunately, the `utiml` package uses the package we used for SVM in this class, `e1071`. After further analysis, SVM was selected.

In the paper “Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes” published in 2009 by many authors from multiple institutions, the authors focus on the approaches to improve on current standards for breast cancer prognosis and prediction of chemotherapy benefit by developing a risk model using gene expressions.
Various methods were used such as Samples and Clinical Data, Microarray, Identification of Prototypical Intrinsic Subtype Samples and Genes, Sample Subtype Prediction. A standardized method of classification using a statistically derived gene and sample set validated across multiple cohorts was developed. They split training and testing data for patient cohorts. Their method is also to use microarray and quantitative reverse transcriptase polymerase chain reaction data to develop a 50-gene subtype predictor using data from prototype samples. In the end, they compared four different models for prediction of relapse and used C-index to compare the strength of the models. C-index is calculated for each test set to form the estimate of each model with two sample t test.
They found that all the intrinsic subtypes are present and significant for outcome predictions in cohorts of patients diagnosed with ER tumors. Stratification of the subtypes within HER2clin-positive samples did not show significance in outcome predictions. And also, approximately 10% of breast cancers were classified as normal-like and can be ER tumors and have an intermediate prognosis.

In another paper “Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer” published in 2015, invasive lobular carcinoma (ILC) was focused on as the second most common histologic subtype of invasive breast cancer along with some potential clinical options. Fisher’s test and binary of copy number alterations and mutations were used to analyze selected events.
The authors also used the microarray approach. Limmavoom package helps them perform differential expression analysis on RNA-seq data. It is to estimate the mean-variance relationship in the data to compute an appropriate precision weight for each observation, this could possibly be used in our project. Expression levels and proteins levels data was used for protein differential expression analysis and was assessed by t test as well, like the previous paper. Another procedure that they used is Consensus Cluster Plus Analysis to determine ILC subtypes based on 1000 most differentially expressed genes and a classifier built by CIaNC. They have found that lobular tumors were predominantly classified as LumA and are ER positive tumors. ER status was clinically determined by immunohistochemistry on 120 of 127 ILC cases, with 94% (n = 113) scoring positively. Their studies also showed that mixed histology tumors really tend to resemble ILC and IDC. Moreover, IDC and ILC discriminant molecular features, in particular CDH1 status, could be used to stratify mixed tumors into ILC-like and IDC-like tumor subgroups. The two papers use microarray, t-test, and stratified samplings. Each has some unique methods of classification that we could also try out for our project.
 


# Summary Statistics and Data Processing 

•	705 observations (breast cancer samples)

•	1936 variables/predictors (4 different omics data types)

•	[1:604] are rn - gene expressions 604 <- categorical data

•	[605:1464] are cn - copy number variations 860 

•	[1465:1713] are mu - somatic mutations 249 <- categorical data

•	[1714:1936] are pp - protein levels 223

•	4 outcomes [1937:1940] 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(dplyr))
brca_org = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/mgarbus2/brca_data_w_subtypes.csv")
```

## Data Cleaning

First, we must clean the data. We will clean the data by only keeping observations that satisfy the criteria of having values equal to the levels/outcomes that we are looking for (Positive, Negative, infiltrating ductal carcinoma and infiltrating lobular carcinoma). If they are not satisfied or null, we will remove them.
```{r, warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#remove the 'vital.status'
brca_org <- brca_org[,-1937]
#organize the 4 outcome data
brca_org <- brca_org[((brca_org$histological.type == "infiltrating lobular carcinoma") | (brca_org$histological.type == "infiltrating ductal carcinoma")) & ((brca_org$HER2.Final.Status == "Positive") | (brca_org$HER2.Final.Status == "Negative")) & ((brca_org$ER.Status == "Positive") | (brca_org$ER.Status == "Negative")) & ((brca_org$PR.Status == "Positive") | (brca_org$PR.Status == "Negative")),]
```

Dimension and sample of our cleaned dataset:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
#cleaned data from our github, this should be the same as brca_org
brca_data  = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
brca_data = brca_data[,-(1:2)]
dim(brca_data)
brca_data[1:5,1935:1940]
```

•	There was no missing value in the entire dataset, so there should be no missing values in the continuous predictors as well

```{r, message=FALSE, warning=FALSE}
sum(colSums(is.na(brca_org)))
```

•	There were some outliers in the continuous predictors, we just did a quick test on one of the continuous variables. But we decided to keep them as they are since there are only a few, and we will filter out the dataset later on with the correlation matrix.

```{r, message=FALSE, warning=FALSE}
outfun <- function(x) {
  abs(x-mean(x,na.rm=TRUE)) > 3*sd(x,na.rm=TRUE)
}
sum(outfun(brca_org[,605]))
sum(outfun(brca_org[,1000]))
```

•	We want focus on filtering out the unsignificant continuous variables first, so we used the correlation matrix method. We then dropped the highly correlated columns.

## Use correlation matrix

We will next split our data into outcomes and predictors, then use the correlation method with a cutoff of 0.7 to filter our highly correlated columns: 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(caret))
# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the outcome vars temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
```

```{r, message=FALSE, warning=FALSE}
# only use variances using the corr mat
# Only continuous predictors
non_categorical_brca = cbind(brca_data[1:604], brca_data[1714:1936])
# use correlation matrix
cor_mat = cor(non_categorical_brca)
# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
# drop highly correlated columns
non_categorical_brca =  non_categorical_brca[, -c(cor_list)]
```


# Modeling `PR.Status`

## Modeling with KNN (predict PR.Status)

First, we will use KNN model to predict `PR.Status`. We will use a test train split of 80/20 on the data. We will only be using the non-categorical variables to build the KNN model since we ran into a lot of errors trying to incorporate the categorical variables

We will find the optimal k-value to use, ranging from k=1-10:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(class)
# set the seed to make partition reproducible
set.seed(432)
k_vals = c(1:10)
err_vals = rep(NA, length(k_vals))
## 80% of the sample size
smp_size <- floor(0.8 * nrow(non_categorical_brca))
for(i in 1:length(k_vals)) {
  
  train_ind <- sample(seq_len(nrow(non_categorical_brca)), size = smp_size)
  brca_train <- as.matrix(non_categorical_brca[train_ind, ])
  y = as.matrix(outcomes$PR.Status[train_ind])
  
  brca_test <- as.matrix(non_categorical_brca[-train_ind, ])
  pred = knn(train = brca_train, test = brca_test, cl = y, k = i)
  cmat = table(pred, as.factor(outcomes$PR.Status[-train_ind]))
  # calculate misclassification rate
  err_vals[i] = (cmat[1,2]+cmat[2,1])/sum(cmat)
}
```

Next, we will plot the graph of the classification errors for each k-value and see if we can use the result to determine the best k:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot(k_vals, err_vals, type = "l", xlab = "k-value", ylab = "Classification Error")
title("Classification Error per K-value")
```

As shown from the plot, the k-value with the lowest classification error is k=7, so we will use that for our model.

Next, use K=7 to create our KNN model and display the confusion matrix:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(432)
train_y = as.matrix(outcomes$PR.Status[train_ind])
y = as.matrix(outcomes$PR.Status[train_ind])
pred = knn(train = brca_train, test = brca_test, cl = y, k = 7)
cmat = table(pred, as.factor(outcomes$PR.Status[-train_ind]))
cmat
```

Displaying the accuracy:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
(cmat[1,1]+cmat[2,2])/sum(cmat)
```
The classificaton error for a model with k=7 is 1-`r (cmat[1,1]+cmat[2,2])/sum(cmat)` = `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)`.


## Modeling with Lasso (predict PR.Status)

Next, we will create a Lasso model with 10-fold cross-validation to help create `PR.Status`:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(glmnet))
set.seed(432)
lasso.fit = cv.glmnet(brca_train, train_y, nfolds = 10, alpha = 1, type.measure = "class", family = "binomial")
```

Plotting the fit of our Lasso model:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center'}
par(mfrow = c(1, 2))
    plot(lasso.fit)
    plot(lasso.fit$glmnet.fit, "lambda")
```

We will use the `best_lambda` to use for the prediction on the test data:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
best_lambda = lasso.fit$lambda.min
test_predict = predict(lasso.fit, brca_test, s = best_lambda, type = "class")
```

Displaying the confusion matrix of the results:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
cmat_1 = table(outcomes$PR.Status[-train_ind], test_predict)
cmat_1
```

Displaying the accuracy:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
(cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)
```
The classification error is 1-`r (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)` = `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`.


### `PR.Status` Summary:

To summarize the model predictions for `PR.Status`, we used KNN and Lasso models to predict the outcome. We only used non-categorical data for both instances since the categorical variables gave many errors while we were in the process of fitting the models. However, even without the categorical predictors, both models yielded relatively low classification errors, with the KNN model (using k=7) having a classification error of `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)` and the Lasso model having a classification error of `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`. Looking at these results, the Lasso model would be better suited to predict `PR.Status`.


# Modelling `histological.type`

Next, let us model `histological.type`, using AUC as the evaluation criterion. For both models that we chose to model `histological.type` with, we decided to use ALL of the categorical and non-categorical predictors (we still used the correlation matrix with cutoff=0.7 to filter out highly correlated columns first) to see if we could get a more accurate result. 

## Modelling `histological.type` with SVM

We decided to test both radial and linear SVM to see which SVM model would perform better.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# SVM, histological type
library(ROCR)
library(e1071)

brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
non_response_brca <- brca_data[,-c(which(colnames(brca_data) %in% c('PR.Status','ER.Status','HER2.Final.Status','histological.type')))]
set.seed(432)

#SVM WITH MATCORR DATA
# use correlation matrix
cor_mat = cor(non_response_brca)
# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
non_response_brca <- non_response_brca[,cor_list]

#New BRCA data
train = brca_data$histological.type
train_data_unsplit = cbind(train,non_response_brca)
#continue to use the combined 10 data? or use the full data
#train_data_unsplit[,'train']  1 and 2
#80 20 split

train_rows <- sample(nrow(train_data_unsplit), size = floor(0.8 *nrow(train_data_unsplit)))
train_data <- train_data_unsplit[train_rows,]
test_data = train_data_unsplit[-train_rows,]
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Let's consider a few different kernels
#Sigma is the distance from the boundary
#Do we want a high cost? Or a low one?
svm.radial <- train(train ~ ., data = train_data, method = "svmRadial",
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1), sigma = c(1, 2, 3)),
                trControl = trainControl(method = "cv", number = 5))
svm.linear <- train(train ~ ., data = train_data, method = 'svmLinear',
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1)),
                trControl = trainControl(method = "cv", number = 5))
```

Let us display the SVM results for radial and linear models respectively:
```{r, message=FALSE, warning=FALSE}
svm.radial
svm.linear
```

Next, we will use each model to predict on the test data:
```{r, message=FALSE, warning=FALSE}
linear_svm_prediction <- predict(svm.linear, test_data[,-1])
radial_svm_prediction <- predict(svm.radial, test_data[,-1])
#Radial --> unable to classify any infiltrating lobular carcinoma. Linear is the better model
suppressMessages(library(ROCR))
lin.svm <- prediction(as.numeric(as.factor(linear_svm_prediction)), as.factor(test_data[,1]))
rad.svm <- prediction(as.numeric(as.factor(radial_svm_prediction)), as.factor(test_data[,1]))
```

Finally, let us display the results of the Linear and Radial SVM AUC:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
# Linear has a performance of about 65.98%
performance(lin.svm,measure = "auc")@y.values[[1]]
# Radial has performance of about 50%
performance(rad.svm,measure = "auc")@y.values[[1]]
```

Looking at the results above, Linear SVM has AUC = `r performance(lin.svm,measure = "auc")@y.values[[1]]` and Radial SVM has AUC = `r performance(rad.svm,measure = "auc")@y.values[[1]]`. We can conclude that Linear SVM is probably the better model due to this significant difference.

## Modelling `histological.type` with Random Forests

Next, let us use a Random Forest model:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
train_y_hist= as.matrix(outcomes$histological.type[train_ind])
brca_train_2 = as.matrix(cbind(brca_train, train_y_hist))
```

Let us fit the model with `nodesize=8` and `sampsize=50`. Through multiple rounds of experimentation with values, we found those parameters to be most optimal:
```{r, message=FALSE, warning=FALSE} 
suppressMessages(library(randomForest))
set.seed(432)
rf.fit = randomForest(data.matrix(brca_train_2[, -c(640)]), y = as.factor(brca_train_2[, 640]), ntree = 4, mtry = 4, nodesize = 8, sampsize = 50)
```

Next, use this model to predict on the test data:
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center'} 
test.predictions <- predict(rf.fit, type="prob", newdata=brca_test)[,2]
correct_hist = as.factor(outcomes$histological.type[-train_ind])
rf_pr_test <- prediction(test.predictions, correct_hist)
r_auc_test <- performance(rf_pr_test, "tpr","fpr")
plot(r_auc_test, colorize=TRUE)
```

The AUC:
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
performance(rf_pr_test, measure = "auc")@y.values[[1]] 
```
The AUC for the random forest model is `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. 

### `histological.type` Summary:

To summarize our models, we used Linear and Radial SVM, which had AUC values of `r performance(lin.svm,measure = "auc")@y.values[[1]]` and `r performance(rad.svm,measure = "auc")@y.values[[1]]` respectively. For random forest, the AUC value resulted in `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. Therefore, the random forest model can be determined as the better model for predicting `histological.type`, although its AUC score is still a bit low.


# 50-Variable Selection

For the 50-variable selection, we decided to...TODO

Finally, let us display our 3-fold cross-validated and averaged AUC:
```{r, message=FALSE, warning=FALSE}
library(utiml)
library(mldr)
library(kableExtra)
#install.packages('xgboost')
library(xgboost)
response_data <- outcomes
response_data_n <- data.frame(sapply(response_data, function(x) as.numeric(as.factor(x)) - 1))
# Create two partitions (train and test) of toyml multi-label dataset
#with_factors 
factor_data <- cbind(non_response_brca[1:50],response_data_n)
my_mldr <- mldr_from_dataframe(factor_data, labelIndices = c(51:54))
#Keep as BR to ignore between labels!!!
svm_results <- cv(my_mldr, br, base.algorith="SVM", seed=432, kernel = "linear", cost = 0.6, cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)
xgb_results <- cv(my_mldr, br, base.algorith="XGB", seed=432, cores = 1, eta = 0.3, nrounds = 50, eval_metric = "auc", cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)
svm_final_result <-rbind(svm_results$multilabel[,c(1,5)],colMeans(svm_results$multilabel[,c(1,5)]))
svfr <- as.table(svm_final_result)
kable(svfr)
```
