---
title: "Final Project Document"
author: "Duyen Ho, Michael Garbus, Christina Hu"
date: "12/6/2021"
output:
  pdf_document:
    includes:
      in_header: header.tex
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
urlcolor: blue
---


\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Description and Summary (NEED GOAL, APPROACH, and CONCLUSION AND THIS NEEDS TO BE ONE PAGE?!!?)

The goal of this project is to find the most accurate model using a variety of statistical model building techniques in order to predict medical outcomes relating to breast cancer. We will use the BRCA Multi-Omics (TCGA) data from Kaggle, and predict outcomes for the `PR.Status`, `ER.Status`, `HER2.Final.Status`, and `histological.type`. For our approach, we will first perform the necessary data cleaning operations, then [method 1] and [method2] to build classification models for `PR.Status` and [method1] and [method2] for `histological.type`, using classification error and AUC respectively as the evalution criteria. Next, we will use [method] to build a model with the goal of accurately predicting **all four** outcomes. Finally, [INSERT CONCLUSION HERE].

- can include any difficulties or describe outcomes for models

# Literature Review

[paste from Michael]

# Summary Statistics and Data Processing

[add more from Duyen]

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
# head(brca_data, 10)
```


Structure of data:
```{r}
#str(brca_data)
```

PR.Status, ER.Status, and HER2.Final.Status are determined using immunohistochemistry scoring. For these variables, we will only consider two levels: “Positive” and “Negative”. For histological.type, we will only consider “infiltrating lobular carcinoma” and “infiltrating ductal carcinoma”. You can treat all other categories as missing values. Hence, all four outcomes should be binary.

```{r}
which(colnames(brca_data)=="PR.Status")
which(colnames(brca_data)=="ER.Status")
which(colnames(brca_data)=="HER2.Final.Status")
which(colnames(brca_data)=="histological.type")
```


Check for null values:
```{r}
colSums(is.na(brca_data[, 1937:1940]))
```


-- check for closely correlated variables and leave only one

```{r}
suppressMessages(library(caret))
suppressMessages(library(dplyr))
# store the predictors
predictors = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))


# use correlation matrix
#cor_mat = cor(brca_data)
```

```{r}
# returns vector of indices to remove
#cor_list = findCorrelation(cor_mat, cutoff = .8)
```

Drop the highly correlated columns
```{r}
#brca_data =  brca_data[, -c(cor_list)]
#brca_data
```
We still have 1087 col left...

```{r}
# categorical variables
# brca_data[605:1464]
# brca_data[1465:1713]
```

```{r}
# only use variances using the corr mat
non_categorical_brca = cbind(brca_data[1:604], brca_data[1714:1936])

# use correlation matrix
cor_mat = cor(non_categorical_brca)

# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
```

```{r}
non_categorical_brca =  non_categorical_brca[, -c(cor_list)]
```

```{r}
# non_categorical_brca
```

# Modeling PR.Status

## Modeling with KNN (predict PR.Status)

First, we will use KNN model to predict `PR.Status`. We will use a test train split of 25/75. We will only be using the non-categorical variables to build the KNN model since we ran into a lot of errors trying to incorporate the categorical variables

We will find the optimal k-value to use, ranging from k=1-10:
```{r}
library(caret)
library(class)

# set the seed to make partition reproducible
set.seed(432)

k_vals = c(1:10)
err_vals = rep(NA, length(k_vals))

## 75% of the sample size
smp_size <- floor(0.75 * nrow(non_categorical_brca))

for(i in 1:length(k_vals)) {
  
  train_ind <- sample(seq_len(nrow(non_categorical_brca)), size = smp_size)

  brca_train <- as.matrix(non_categorical_brca[train_ind, ])
  y = as.matrix(predictors$PR.Status[train_ind])
  
  brca_test <- as.matrix(non_categorical_brca[-train_ind, ])

  pred = knn(train = brca_train, test = brca_test, cl = y, k = i)
  cmat = table(pred, as.factor(predictors$PR.Status[-train_ind]))
  # calculate misclassification rate
  err_vals[i] = (cmat[1,2]+cmat[2,1])/sum(cmat)
}
```

Next, we will plot the graph of the classification errors for each k-value and see if we can use the result to determine the best k:

```{r}
plot(k_vals, err_vals, type = "l", xlab = "k-value", ylab = "Classification Error")
title("Classification Error per K-value")
```

As shown from the plot, the k-value with the lowest classification error is k=5, so we will use that for our model.

Next, use K=5 to create our KNN model and display the confusion matrix:
```{r}
set.seed(432)
train_y = as.matrix(predictors$PR.Status[train_ind])
y = as.matrix(predictors$PR.Status[train_ind])
pred = knn(train = brca_train, test = brca_test, cl = y, k = 5)
cmat = table(pred, as.factor(predictors$PR.Status[-train_ind]))
cmat
```

Displaying the accuracy:
```{r}
(cmat[1,1]+cmat[2,2])/sum(cmat)
```
The classificaton error for a model with k=5 is `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)`.


## Modeling with Lasso (predict PR.Status)

Next, we will create a Lasso model with 10-fold cross-validation to help create `PR.Status`:
```{r}
suppressMessages(library(glmnet))
set.seed(432)

lasso.fit = cv.glmnet(brca_train, train_y, nfolds = 10, alpha = 1, type.measure = "class", family = "binomial")
```

Plotting the fit of our Lasso model:
```{r}
par(mfrow = c(1, 2))
    plot(lasso.fit)
    plot(lasso.fit$glmnet.fit, "lambda")
```

We will use the `best_lambda` to use for the prediction on the test data:
```{r}
best_lambda = lasso.fit$lambda.min
test_predict = predict(lasso.fit, brca_test, s = best_lambda, type = "class")
```

Displaying the confusion matrix of the results:
```{r}
cmat_1 = table(predictors$PR.Status[-train_ind], test_predict)
cmat_1
```

Displaying the accuracy:
```{r}
(cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)
```
The classification error is `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`


### `PR.Status` Summary:

To summarize the model predictions for `PR.Status`, we used KNN and Lasso models to predict the outcome. We only used non-categorical data for both instances since the categorical variables gave many errors while we were in the process of fitting the models. However, even without the categorical predictors, both models yielded relatively low classification errors, with the KNN model (using k=8) having a classification error of `r 1-(cmat[1,1]+cmat[2,2])/sum(cmat)` and the Lasso model having a classification error of `r 1- (cmat_1[1,1]+cmat_1[2,2])/sum(cmat_1)`. Looking at these results, the Lasso model would be better suited to predict `PR.Status`.


# Modelling `histological.type`

Next, let us model `histological.type`, using AUC as the evaluation criterion. For both models that we chose to model `histological.type` with, we decided to use ALL of the categorical and non-categorical predictors to see if we could get a more accurate result. 

## Modelling `histological.type` with SVM

```{r}
# SVM, histological type
library(ROCR)
library(e1071)
library(caret)

brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
non_response_brca <- brca_data[,-c(which(colnames(brca_data) %in% c('PR.Status','ER.Status','HER2.Final.Status','histological.type')))]

set.seed(432)

#SVM WITH MATCORR DATA

# use correlation matrix
cor_mat = cor(non_response_brca)

# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)

non_response_brca <- non_response_brca[,cor_list]

#New BRCA data

train = brca_data$histological.type
train_data_unsplit = cbind(train,non_response_brca)
#continue to use the combined 10 data? or use the full data

#train_data_unsplit[,'train']  1 and 2
#80 20 split
train_rows <- sample(nrow(train_data_unsplit), size = floor(0.8 *nrow(train_data_unsplit)))
train_data <- train_data_unsplit[train_rows,]
test_data = train_data_unsplit[-train_rows,]

#Let's consider a few different kernels
#Sigma is the distance from the boundary
#Do we want a high cost? Or a low one?

svm.radial <- train(train ~ ., data = train_data, method = "svmRadial",
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1), sigma = c(1, 2, 3)),
                trControl = trainControl(method = "cv", number = 5))


svm.linear <- train(train ~ ., data = train_data, method = 'svmLinear',
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1)),
                trControl = trainControl(method = "cv", number = 5))
```

Let us display the SVM results for radial and linear models respectively:
```{r}
svm.radial
svm.linear
```

Next, we will use each model to predict on the test data
```{r}
suppressMessages(library(kableExtra))
linear_svm_prediction <- predict(svm.linear, test_data[,-1])

radial_svm_prediction <- predict(svm.radial, test_data[,-1])

#Radial --> unable to classify any infiltrating lobular carcinoma. Linear is the better model
suppressMessages(library(ROCR))

lin.svm <- prediction(as.numeric(as.factor(linear_svm_prediction)), as.factor(test_data[,1]))

rad.svm <- prediction(as.numeric(as.factor(radial_svm_prediction)), as.factor(test_data[,1]))
```

Finally, let us display the results of the Linear and Radial SVM AUC:
```{r}
# Linear has a performance of about 65.98%
performance(lin.svm,measure = "auc")@y.values[[1]]
# Radial has performance of about 50%
performance(rad.svm,measure = "auc")@y.values[[1]]
```

Looking at the results above, Linear SVM has AUC = `r performance(lin.svm,measure = "auc")@y.values[[1]]` and Radial SVM has AUC = `r `performance(rad.svm,measure = "auc")@y.values[[1]]`. We can conclude that Linear SVM is probably the better model due to this significant difference.

## Modelling `histological.type` with Random Forests

Next, let us use a Random Forest model:
```{r}
train_y_hist= as.matrix(predictors$histological.type[train_ind])
brca_train_2 = as.matrix(cbind(brca_train, train_y_hist))
```

Let us fit the model with `nodesize=8` and `sampsize=50`. Through multiple rounds of experimentation with values, we found those parameters to be most optimal:
```{r}
suppressMessages(library(randomForest))
set.seed(432)
rf.fit = randomForest(data.matrix(brca_train_2[, -c(640)]), y = as.factor(brca_train_2[, 640]), ntree = 4, mtry = 4, nodesize = 8, sampsize = 50)
```

Next, use this model to predict on the test data:
```{r}
test.predictions <- predict(rf.fit, type="prob", newdata=brca_test)[,2]

correct_hist = as.factor(predictors$histological.type[-train_ind])
rf_pr_test <- prediction(test.predictions, correct_hist)
r_auc_test <- performance(rf_pr_test, "tpr","fpr")
plot(r_auc_test, colorize=TRUE)
```

The AUC:
```{r}
performance(rf_pr_test, measure = "auc")@y.values[[1]] 
```
The `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. 

### `histological.type` Summary:
To summarize our models, we used Linear and Radial SVM, with had AUC values of `r performance(lin.svm,measure = "auc")@y.values[[1]]` and `r performance(rad.svm,measure = "auc")@y.values[[1]]` respectively. For random forest, the AUC value resulted in `r performance(rf_pr_test, measure = "auc")@y.values[[1]] `. Therefore, the random forest model can be determined as the better model, although its AUC score is still a bit low.


# 50-Variable Selection

## PR.Status
```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
PR.Status = as.numeric(as.factor(outcomes$PR.Status))-1
pr_data = cbind(PR.Status, brca_data)

pr_data[sapply(pr_data, is.character)] <- lapply(pr_data[sapply(pr_data, is.character)], as.numeric)


# head(pr_data)

M <- cor(pr_data)
correlation_threshold <- 0.446
significant_correlation <- abs(M["PR.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```


```{r}
suppressMessages(library(caret))
# Linear regression model with selected features, 10 K-fold cross validation using the "caret" package --------------
set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(pr_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(pr_data)), size = smp_size)

train_y <- outcomes$PR.Status[train_ind]
pr_train <- cbind(train_y, pr_data[train_ind, ])
pr_train <- pr_train[, corr_colnames]

# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(PR.Status) ~ ., data=pr_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
test_y <- outcomes$PR.Status[-train_ind]
pr_test <- cbind(test_y, pr_data[-train_ind, ])

# length(test_y)
# dim(pr_data[-train_ind,])

test_predict <- predict(mod_fit, pr_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```
The AUC for PR.Status is 0.8989979

## Histological.Type

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
histological.type = as.numeric(as.factor(outcomes$histological.type))

hist_data = cbind(histological.type, brca_data)

# head(hist_data,20)

hist_data[sapply(hist_data, is.character)] <- lapply(hist_data[sapply(hist_data, is.character)], as.numeric)
```

```{r}
M <- cor(hist_data)
correlation_threshold <- 0.204
significant_correlation <- abs(M["histological.type", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(hist_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(hist_data)), size = smp_size)


train_y <- outcomes$histological.type[train_ind]
hist_train <- cbind(train_y, hist_data[train_ind, ])
hist_train <- hist_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(histological.type) ~ ., data=hist_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
test_y <- as.data.frame(outcomes$histological.type[-train_ind])

dim(test_y)
dim(hist_data[-train_ind,])

hist_test <- cbind(test_y, hist_data[-train_ind, ])

test_predict <- predict(mod_fit, hist_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```

The AUC for histological.type is 0.9318182


## ER.Status

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
ER.Status = as.numeric(as.factor(outcomes$ER.Status))

er_data = cbind(ER.Status, brca_data)


er_data[sapply(er_data, is.character)] <- lapply(er_data[sapply(er_data, is.character)], as.numeric)
```

```{r}
M <- cor(er_data)
correlation_threshold <- 0.501
significant_correlation <- abs(M["ER.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r, warning=FALSE}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(er_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(er_data)), size = smp_size)


train_y <- outcomes$ER.Status[train_ind]
er_train <- cbind(train_y, er_data[train_ind, ])
er_train <- er_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(ER.Status) ~ ., data=er_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
library(pROC)
library(ROCR)

test_y <- as.data.frame(outcomes$ER.Status[-train_ind])

dim(test_y)
dim(er_data[-train_ind,])

er_test <- cbind(test_y, er_data[-train_ind, ])

test_predict <- predict(mod_fit, er_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```
The AUC for ER.Status is 0.9318182

## HER2.Final.Status

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
HER2.Final.Status = as.numeric(as.factor(outcomes$HER2.Final.Status))

final_status_data = cbind(HER2.Final.Status, brca_data)


final_status_data[sapply(final_status_data, is.character)] <- lapply(final_status_data[sapply(final_status_data, is.character)], as.numeric)
```

```{r}
M <- cor(final_status_data)
correlation_threshold <- 0.193
significant_correlation <- abs(M["HER2.Final.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r, warning=FALSE}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(final_status_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(final_status_data)), size = smp_size)


train_y <- outcomes$HER2.Final.Status[train_ind]
final_status_train <- cbind(train_y, final_status_data[train_ind, ])
final_status_train <- final_status_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(HER2.Final.Status) ~ ., data=final_status_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
test_y <- as.data.frame(outcomes$HER2.Final.Status[-train_ind])

dim(test_y)
dim(final_status_data[-train_ind,])

final_status_test <- cbind(test_y, final_status_data[-train_ind, ])

test_predict <- predict(mod_fit, final_status_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```

The AUC for HER2.Final.Status is 0.8743316

Now, to select the actual 50-variables.

1) Output all of the 50 predictor variables for each outcome

```{r}

```

2) Generate a count for top ones
3) pick the top 50 from there






