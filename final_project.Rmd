---
title: "Final Project Document"
author: "Duyen Ho, Michael Garbus, Christina Hu"
date: "12/6/2021"
output:
  pdf_document:
    toc_depth: 2
urlcolor: blue
---

<center>

#Todo: shrink to height, add TOC.
{#id .class width=30%, height=30%}


![UIUC Logo](images/1200px-University_of_Illinois_seal.svg.png)\



</center>




<<<<<<< HEAD
\newpage
=======
However, the highest values for AUC were KNN and a random forest model, both achieving an AUC of $99\%$. 
>>>>>>> 15f97385675b2099e3ad2c2384813a6fe4d81b21


<<<<<<< HEAD

=======
A similar study, "Using Machine Learning Algorithms for Breast Cancer Risk
Prediction and Diagnosis", published in 2016 by Asri,H. et al in Procedia Computer Science 83 tested SVM, decision tree (C 4.5, a technique available in the open source WEKA data analysis tool), Naive Bayes, as well as KNN on the same UCI Breast Cancer dataset, with a focus on accuracy. Asri et al. discovered that although SVM took the longest time to create a model, it resulted in the highest accuracy of $97.54\%$,  which is a different result to the previous model we studied. 

While I do not believe that time constraints will be important in regards to our model as we are trying to achieve a high level of classification, it is interesting that these values are different. I imagine that this may be due to a difference in tools used, or perhaps, in the kernel used for the SVM classification. Additionally, the study also found that SVM had the highest AUC value on the ROC curve ($99\%$). For these reason, we decided to use SVM for our model.

For the creation of our final model, we consulted some literature on predicting in medical data. We found "Predicting Missing Values in Medical Data via XGBoost Regression", from Zhang et al. Because of this model, as well as discussion in Islam et al., we decided to consider XGBoost. Fortunately, XGBoost is able to use AUC values as its objective function. We decided to use a multi-label machine learning tool, `utiml`, so we would be able to predict all four labels from our 50 predictors. Serendipitously, this library is very similar to the WEKA tool used in Asri et al. which led us to consider using SVM for this application as well. Fortunately, the `utiml` package uses the package we used for SVM in this class, `e1071`. After further analysis, SVM was selected.
>>>>>>> 15f97385675b2099e3ad2c2384813a6fe4d81b21


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Description and Summary (NEED GOAL, APPROACH, and CONCLUSION AND THIS NEEDS TO BE ONE PAGE?!!?)

The goal of this project is to find the most accurate model using a variety of statistical model building techniques in order to predict medical outcomes relating to breast cancer. We will use the BRCA Multi-Omics (TCGA) data from Kaggle, and predict outcomes for the `PR.Status`, `ER.Status`, `HER2.Final.Status`, and `histological.type`. For our approach, we will first perform the necessary data cleaning operations, then [method 1] and [method2] to build classification models for `PR.Status` and [method1] and [method2] for `histological.type`, using classification error and AUC respectively as the evalution criteria. Next, we will use [method] to build a model with the goal of accurately predicting **all four** outcomes. Finally, [INSERT CONCLUSION HERE].

- can include any difficulties or describe outcomes for models

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))
head(brca_data, 10)
```


Structure of data:
```{r}
#str(brca_data)
```

PR.Status, ER.Status, and HER2.Final.Status are determined using immunohistochemistry scoring. For these variables, we will only consider two levels: “Positive” and “Negative”. For histological.type, we will only consider “infiltrating lobular carcinoma” and “infiltrating ductal carcinoma”. You can treat all other categories as missing values. Hence, all four outcomes should be binary.

```{r}
which(colnames(brca_data)=="PR.Status")
which(colnames(brca_data)=="ER.Status")
which(colnames(brca_data)=="HER2.Final.Status")
which(colnames(brca_data)=="histological.type")
```


Check for null values:
```{r}
colSums(is.na(brca_data[, 1937:1940]))
```


-- check for closely correlated variables and leave only one

```{r}
suppressMessages(library(caret))
suppressMessages(library(dplyr))
# store the predictors
predictors = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))


# use correlation matrix
#cor_mat = cor(brca_data)
```

```{r}
# returns vector of indices to remove
#cor_list = findCorrelation(cor_mat, cutoff = .8)
```

Drop the highly correlated columns
```{r}
#brca_data =  brca_data[, -c(cor_list)]
#brca_data
```
We still have 1087 col left...

```{r}
# categorical variables
brca_data[605:1464]
brca_data[1465:1713]
```

```{r}
# only use variances using the corr mat
non_categorical_brca = cbind(brca_data[1:604], brca_data[1714:1936])

# use correlation matrix
cor_mat = cor(non_categorical_brca)

# returns vector of indices to remove
cor_list = findCorrelation(cor_mat, cutoff = .7)
```

```{r}
non_categorical_brca =  non_categorical_brca[, -c(cor_list)]
```

```{r}
non_categorical_brca
```

# Modeling with KNN (predict PR.Status)

Split the data:
```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(non_categorical_brca))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(non_categorical_brca)), size = smp_size)

brca_train <- as.matrix(non_categorical_brca[train_ind, ])
brca_test <- as.matrix(non_categorical_brca[-train_ind, ])


train_y = as.matrix(predictors$PR.Status[train_ind])
brca_train_1 = as.matrix(cbind(brca_train, train_y))
```
The best k is 10

```{r}
library(caret)
library(class)
train_y = as.matrix(predictors$PR.Status[train_ind])

y = as.matrix(predictors$PR.Status[train_ind])

pred = knn.fit = knn(train = brca_train, test = brca_test, cl = y, k = 10)

cmat = table(pred, as.factor(predictors$PR.Status[-train_ind]))

cmat
```

Accuracy:
```{r}
(81 + 23)/sum(cmat)
```

# Modeling with Lasso (predict PR.Status)
```{r}
suppressMessages(library(glmnet))
set.seed(1)

lasso.fit = cv.glmnet(brca_train, train_y, nfolds = 10, alpha = 1, type.measure = "class", family = "binomial")

par(mfrow = c(1, 2))
    plot(lasso.fit)
    plot(lasso.fit$glmnet.fit, "lambda")
```
# ```{r}
# coef(lasso.fit, s = "lambda.min")
# ```

# ```{r}
# coef(lasso.fit, s = "lambda.1se")
# ```

Predict on the test data:
```{r}
best_lambda = lasso.fit$lambda.min
test_predict = predict(lasso.fit, brca_test, s = best_lambda, type = "class")

cmat_1 = table(predictors$PR.Status[-train_ind], test_predict)
cmat_1
```

Accuracy:
```{r}
(25 + 79)/sum(cmat_1)
```
The classification error is `r 1- (25 + 79)/sum(cmat_1)`

# Modelling `histological.type` with Logistic (with Ridge and Lasso penalty)

Next, let us model `histological.type`, using AUC as the evaluation criterion. 

First, use Penalized Logistic Regression:

```{r}
library(glmnet)
require(methods)

train_y_hist= as.matrix(predictors$histological.type[train_ind])
brca_train_2 = as.matrix(cbind(brca_train, train_y_hist))

# need to force matrix as dgCMatrix for some reason??: https://stackoverflow.com/questions/8458233/r-glmnet-as-matrix-error-message

# using the train data and 10-fold CV
logistic.fit = cv.glmnet(x = as(data.matrix(brca_train_2[, -c(640)]), "dgCMatrix"), y = brca_train_2[, 640], nfold = 10, family = "binomial")

plot(logistic.fit)
```


Next, use this model to predict on the test data:
```{r}
library(ROCR)
test_predict <- predict(logistic.fit, brca_test, type="response")


roc <- prediction(test_predict, predictors$histological.type[-train_ind])

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)
```

The AUC:
```{r}
performance(roc, measure = "auc")@y.values[[1]]
```


Next, try with Ridge:
```{r}
library(glmnet)
set.seed(432)
ridge.fit = cv.glmnet(as(data.matrix(brca_train_2[, -c(640)]), "dgCMatrix"), y = brca_train_2[, 640], nfolds = 10, alpha = 0, family = "binomial")
plot(ridge.fit$glmnet.fit, "lambda")
```

Next, use this model to predict on the test data:
```{r}
library(ROCR)
test_predict <- predict(ridge.fit, brca_test, type="response")

roc <- prediction(test_predict, predictors$histological.type[-train_ind])

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)
```

The AUC:
```{r}
performance(roc, measure = "auc")@y.values[[1]]
```

The AUC for logistic regression with ridge penalty is lower than the lasso penalty.


# Modelling `histological.type` with Kernel

```{r}
library(randomForest)
set.seed(432)
rf.fit = randomForest(data.matrix(brca_train_2[, -c(640)]), y = as.factor(brca_train_2[, 640]), ntree = 4, mtry = 4, nodesize = 10, sampsize = 50)
```

Next, use this model to predict on the test data:
```{r}
library(ROCR)
library(pROC)
test.predictions <- predict(rf.fit, type="prob", newdata=brca_test)[,2]

correct_hist = as.factor(predictors$histological.type[-train_ind])
rf_pr_test <- prediction(test.predictions, correct_hist)
r_auc_test <- performance(rf_pr_test, "tpr","fpr")
plot(r_auc_test, colorize=TRUE)
```

The AUC:
```{r}
performance(rf_pr_test, measure = "auc")@y.values[[1]] 
```

this is bad



# 50-Variable Selection

## PR.Status
```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
PR.Status = as.numeric(as.factor(outcomes$PR.Status))-1
pr_data = cbind(PR.Status, brca_data)

pr_data[sapply(pr_data, is.character)] <- lapply(pr_data[sapply(pr_data, is.character)], as.numeric)


# head(pr_data)

M <- cor(pr_data)
correlation_threshold <- 0.446
significant_correlation <- abs(M["PR.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```


```{r}
library(caret)
# Linear regression model with selected features, 10 K-fold cross validation using the "caret" package --------------
set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(pr_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(pr_data)), size = smp_size)

train_y <- outcomes$PR.Status[train_ind]
pr_train <- cbind(train_y, pr_data[train_ind, ])
pr_train <- pr_train[, corr_colnames]

# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(PR.Status) ~ ., data=pr_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
library(pROC)
library(ROCR)

test_y <- outcomes$PR.Status[-train_ind]
pr_test <- cbind(test_y, pr_data[-train_ind, ])

# length(test_y)
# dim(pr_data[-train_ind,])

test_predict <- predict(mod_fit, pr_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```
The AUC for PR.Status is 0.8989979

## Histological.Type

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
histological.type = as.numeric(as.factor(outcomes$histological.type))

hist_data = cbind(histological.type, brca_data)

# head(hist_data,20)

hist_data[sapply(hist_data, is.character)] <- lapply(hist_data[sapply(hist_data, is.character)], as.numeric)
```

```{r}
M <- cor(hist_data)
correlation_threshold <- 0.204
significant_correlation <- abs(M["histological.type", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(hist_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(hist_data)), size = smp_size)


train_y <- outcomes$histological.type[train_ind]
hist_train <- cbind(train_y, hist_data[train_ind, ])
hist_train <- hist_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(histological.type) ~ ., data=hist_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
library(pROC)
library(ROCR)

test_y <- as.data.frame(outcomes$histological.type[-train_ind])

dim(test_y)
dim(hist_data[-train_ind,])

hist_test <- cbind(test_y, hist_data[-train_ind, ])

test_predict <- predict(mod_fit, hist_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```

The AUC for histological.type is 0.9318182


## ER.Status

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
ER.Status = as.numeric(as.factor(outcomes$ER.Status))

er_data = cbind(ER.Status, brca_data)


er_data[sapply(er_data, is.character)] <- lapply(er_data[sapply(er_data, is.character)], as.numeric)
```

```{r}
M <- cor(er_data)
correlation_threshold <- 0.501
significant_correlation <- abs(M["ER.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(er_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(er_data)), size = smp_size)


train_y <- outcomes$ER.Status[train_ind]
er_train <- cbind(train_y, er_data[train_ind, ])
er_train <- er_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(ER.Status) ~ ., data=er_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
library(pROC)
library(ROCR)

test_y <- as.data.frame(outcomes$ER.Status[-train_ind])

dim(test_y)
dim(er_data[-train_ind,])

er_test <- cbind(test_y, er_data[-train_ind, ])

test_predict <- predict(mod_fit, er_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```
The AUC for ER.Status is 0.9318182

## HER2.Final.Status

```{r}
library(dplyr)
brca_data = read.csv("https://raw.githubusercontent.com/mgarbvs/STAT-432-final-project/main/cleaned_brca_data.csv")
# remove X.1 and X columns
brca_data = brca_data %>% select(-c("X.1", "X"))

# store the outcomes
outcomes = brca_data %>% select(c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))
# remove the categorical var temporarily
brca_data = brca_data %>% select(-c("PR.Status", "ER.Status", "HER2.Final.Status", "histological.type"))

# Create a correlation matrix and select only variables with some correlation to the target
HER2.Final.Status = as.numeric(as.factor(outcomes$HER2.Final.Status))

final_status_data = cbind(HER2.Final.Status, brca_data)


final_status_data[sapply(final_status_data, is.character)] <- lapply(final_status_data[sapply(final_status_data, is.character)], as.numeric)
```

```{r}
M <- cor(final_status_data)
correlation_threshold <- 0.193
significant_correlation <- abs(M["HER2.Final.Status", ]) > correlation_threshold
table(significant_correlation)

corr_colnames <- colnames(M)[significant_correlation]
```

```{r}
library(caret)

set.seed(432)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(final_status_data))

## set the seed to make your partition reproducible
train_ind <- sample(seq_len(nrow(final_status_data)), size = smp_size)


train_y <- outcomes$HER2.Final.Status[train_ind]
final_status_train <- cbind(train_y, final_status_data[train_ind, ])
final_status_train <- final_status_train[, corr_colnames]


# Training parameters
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = TRUE,
                     classProbs = FALSE)

# Fitting model
mod_fit <- train(as.factor(HER2.Final.Status) ~ ., data=final_status_train,
                 method="glm",
                 family="binomial",
                 trControl = ctrl,
                 tuneLength = 5)
```

```{r}
summary(mod_fit)
```

Compute and plot the AUC:
```{r}
library(pROC)
library(ROCR)

test_y <- as.data.frame(outcomes$HER2.Final.Status[-train_ind])

dim(test_y)
dim(final_status_data[-train_ind,])

final_status_test <- cbind(test_y, final_status_data[-train_ind, ])

test_predict <- predict(mod_fit, final_status_test, type="prob")
roc <- prediction(test_predict[,2], test_y)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")
plot(perf,colorize=TRUE)

# obtaining the AUC
performance(roc, measure = "auc")@y.values[[1]] 
```

The AUC for HER2.Final.Status is 0.8743316



<<<<<<< HEAD



=======
```{r}
library(utiml)
library(mldr)
library(kableExtra)
# Create two partitions (train and test) of toyml multi-label dataset
#with_factors 
factor_data <- cbind(non_response_brca[1:50],response_data_n)
my_mldr <- mldr_from_dataframe(factor_data, labelIndices = c(51:54))

#Keep as BR to ignore between labels!!!
svm_results <- cv(my_mldr, br, base.algorith="SVM", seed=432, kernel = "linear", cost = 0.6, cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)

xgb_results <- cv(my_mldr, br, base.algorith="XGB", seed=432, cores = 1, eta = 0.3, nrounds = 50, eval_metric = "auc", cv.folds=3, cv.sampling="random", cv.measures=c("macro-based", "micro-based"), cv.seed=432,cv.results=TRUE)

svm_final_result <-rbind(svm_results$multilabel[,c(1,5)],colMeans(svm_results$multilabel[,c(1,5)]))
svfr <- as.table(svm_final_result)

kable(svfr)




```
>>>>>>> 15f97385675b2099e3ad2c2384813a6fe4d81b21

